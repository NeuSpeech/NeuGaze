7767517
93 92
pnnx.Input               pnnx_input_0             0 1 0 #0=(1,3,448,448)f32
nn.Conv2d                stage0.reparam_conv      1 1 0 1 bias=True dilation=(1,1) groups=1 in_channels=3 kernel_size=(3,3) out_channels=48 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(48)f32 @weight=(48,3,3,3)f32 #0=(1,3,448,448)f32 #1=(1,48,224,224)f32
nn.ReLU                  stage0.activation        1 1 1 2 #1=(1,48,224,224)f32 #2=(1,48,224,224)f32
nn.Conv2d                stage1.0.reparam_conv    1 1 2 3 bias=True dilation=(1,1) groups=48 in_channels=48 kernel_size=(3,3) out_channels=48 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(48)f32 @weight=(48,1,3,3)f32 #2=(1,48,224,224)f32 #3=(1,48,112,112)f32
nn.ReLU                  stage1.0.activation      1 1 3 4 #3=(1,48,112,112)f32 #4=(1,48,112,112)f32
nn.Conv2d                stage1.1.reparam_conv    1 1 4 5 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(1,1) out_channels=48 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(48)f32 @weight=(48,48,1,1)f32 #4=(1,48,112,112)f32 #5=(1,48,112,112)f32
nn.ReLU                  stage1.1.activation      1 1 5 6 #5=(1,48,112,112)f32 #6=(1,48,112,112)f32
nn.Conv2d                stage1.2.reparam_conv    1 1 6 7 bias=True dilation=(1,1) groups=48 in_channels=48 kernel_size=(3,3) out_channels=48 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(48)f32 @weight=(48,1,3,3)f32 #6=(1,48,112,112)f32 #7=(1,48,112,112)f32
nn.ReLU                  stage1.2.activation      1 1 7 8 #7=(1,48,112,112)f32 #8=(1,48,112,112)f32
nn.Conv2d                stage1.3.reparam_conv    1 1 8 9 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(1,1) out_channels=48 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(48)f32 @weight=(48,48,1,1)f32 #8=(1,48,112,112)f32 #9=(1,48,112,112)f32
nn.ReLU                  stage1.3.activation      1 1 9 10 #9=(1,48,112,112)f32 #10=(1,48,112,112)f32
nn.Conv2d                stage2.0.reparam_conv    1 1 10 11 bias=True dilation=(1,1) groups=48 in_channels=48 kernel_size=(3,3) out_channels=48 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(48)f32 @weight=(48,1,3,3)f32 #10=(1,48,112,112)f32 #11=(1,48,56,56)f32
nn.ReLU                  stage2.0.activation      1 1 11 12 #11=(1,48,56,56)f32 #12=(1,48,56,56)f32
nn.Conv2d                stage2.1.reparam_conv    1 1 12 13 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,48,1,1)f32 #12=(1,48,56,56)f32 #13=(1,128,56,56)f32
nn.ReLU                  stage2.1.activation      1 1 13 14 #13=(1,128,56,56)f32 #14=(1,128,56,56)f32
nn.Conv2d                stage2.2.reparam_conv    1 1 14 15 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #14=(1,128,56,56)f32 #15=(1,128,56,56)f32
nn.ReLU                  stage2.2.activation      1 1 15 16 #15=(1,128,56,56)f32 #16=(1,128,56,56)f32
nn.Conv2d                stage2.3.reparam_conv    1 1 16 17 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #16=(1,128,56,56)f32 #17=(1,128,56,56)f32
nn.ReLU                  stage2.3.activation      1 1 17 18 #17=(1,128,56,56)f32 #18=(1,128,56,56)f32
nn.Conv2d                stage2.4.reparam_conv    1 1 18 19 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #18=(1,128,56,56)f32 #19=(1,128,56,56)f32
nn.ReLU                  stage2.4.activation      1 1 19 20 #19=(1,128,56,56)f32 #20=(1,128,56,56)f32
nn.Conv2d                stage2.5.reparam_conv    1 1 20 21 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #20=(1,128,56,56)f32 #21=(1,128,56,56)f32
nn.ReLU                  stage2.5.activation      1 1 21 22 #21=(1,128,56,56)f32 #22=(1,128,56,56)f32
nn.Conv2d                stage2.6.reparam_conv    1 1 22 23 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #22=(1,128,56,56)f32 #23=(1,128,56,56)f32
nn.ReLU                  stage2.6.activation      1 1 23 24 #23=(1,128,56,56)f32 #24=(1,128,56,56)f32
nn.Conv2d                stage2.7.reparam_conv    1 1 24 25 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #24=(1,128,56,56)f32 #25=(1,128,56,56)f32
nn.ReLU                  stage2.7.activation      1 1 25 26 #25=(1,128,56,56)f32 #26=(1,128,56,56)f32
nn.Conv2d                stage2.8.reparam_conv    1 1 26 27 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #26=(1,128,56,56)f32 #27=(1,128,56,56)f32
nn.ReLU                  stage2.8.activation      1 1 27 28 #27=(1,128,56,56)f32 #28=(1,128,56,56)f32
nn.Conv2d                stage2.9.reparam_conv    1 1 28 29 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #28=(1,128,56,56)f32 #29=(1,128,56,56)f32
nn.ReLU                  stage2.9.activation      1 1 29 30 #29=(1,128,56,56)f32 #30=(1,128,56,56)f32
nn.Conv2d                stage2.10.reparam_conv   1 1 30 31 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #30=(1,128,56,56)f32 #31=(1,128,56,56)f32
nn.ReLU                  stage2.10.activation     1 1 31 32 #31=(1,128,56,56)f32 #32=(1,128,56,56)f32
nn.Conv2d                stage2.11.reparam_conv   1 1 32 33 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #32=(1,128,56,56)f32 #33=(1,128,56,56)f32
nn.ReLU                  stage2.11.activation     1 1 33 34 #33=(1,128,56,56)f32 #34=(1,128,56,56)f32
nn.Conv2d                stage2.12.reparam_conv   1 1 34 35 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #34=(1,128,56,56)f32 #35=(1,128,56,56)f32
nn.ReLU                  stage2.12.activation     1 1 35 36 #35=(1,128,56,56)f32 #36=(1,128,56,56)f32
nn.Conv2d                stage2.13.reparam_conv   1 1 36 37 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #36=(1,128,56,56)f32 #37=(1,128,56,56)f32
nn.ReLU                  stage2.13.activation     1 1 37 38 #37=(1,128,56,56)f32 #38=(1,128,56,56)f32
nn.Conv2d                stage2.14.reparam_conv   1 1 38 39 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,1,3,3)f32 #38=(1,128,56,56)f32 #39=(1,128,56,56)f32
nn.ReLU                  stage2.14.activation     1 1 39 40 #39=(1,128,56,56)f32 #40=(1,128,56,56)f32
nn.Conv2d                stage2.15.reparam_conv   1 1 40 41 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,1,1)f32 #40=(1,128,56,56)f32 #41=(1,128,56,56)f32
nn.ReLU                  stage2.15.activation     1 1 41 42 #41=(1,128,56,56)f32 #42=(1,128,56,56)f32
nn.Conv2d                stage3.0.reparam_conv    1 1 42 43 bias=True dilation=(1,1) groups=128 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(128)f32 @weight=(128,1,3,3)f32 #42=(1,128,56,56)f32 #43=(1,128,28,28)f32
nn.ReLU                  stage3.0.activation      1 1 43 44 #43=(1,128,28,28)f32 #44=(1,128,28,28)f32
nn.Conv2d                stage3.1.reparam_conv    1 1 44 45 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,128,1,1)f32 #44=(1,128,28,28)f32 #45=(1,256,28,28)f32
nn.ReLU                  stage3.1.activation      1 1 45 46 #45=(1,256,28,28)f32 #46=(1,256,28,28)f32
nn.Conv2d                stage3.2.reparam_conv    1 1 46 47 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #46=(1,256,28,28)f32 #47=(1,256,28,28)f32
nn.ReLU                  stage3.2.activation      1 1 47 48 #47=(1,256,28,28)f32 #48=(1,256,28,28)f32
nn.Conv2d                stage3.3.reparam_conv    1 1 48 49 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #48=(1,256,28,28)f32 #49=(1,256,28,28)f32
nn.ReLU                  stage3.3.activation      1 1 49 50 #49=(1,256,28,28)f32 #50=(1,256,28,28)f32
nn.Conv2d                stage3.4.reparam_conv    1 1 50 51 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #50=(1,256,28,28)f32 #51=(1,256,28,28)f32
nn.ReLU                  stage3.4.activation      1 1 51 52 #51=(1,256,28,28)f32 #52=(1,256,28,28)f32
nn.Conv2d                stage3.5.reparam_conv    1 1 52 53 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #52=(1,256,28,28)f32 #53=(1,256,28,28)f32
nn.ReLU                  stage3.5.activation      1 1 53 54 #53=(1,256,28,28)f32 #54=(1,256,28,28)f32
nn.Conv2d                stage3.6.reparam_conv    1 1 54 55 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #54=(1,256,28,28)f32 #55=(1,256,28,28)f32
nn.ReLU                  stage3.6.activation      1 1 55 56 #55=(1,256,28,28)f32 #56=(1,256,28,28)f32
nn.Conv2d                stage3.7.reparam_conv    1 1 56 57 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #56=(1,256,28,28)f32 #57=(1,256,28,28)f32
nn.ReLU                  stage3.7.activation      1 1 57 58 #57=(1,256,28,28)f32 #58=(1,256,28,28)f32
nn.Conv2d                stage3.8.reparam_conv    1 1 58 59 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #58=(1,256,28,28)f32 #59=(1,256,28,28)f32
nn.ReLU                  stage3.8.activation      1 1 59 60 #59=(1,256,28,28)f32 #60=(1,256,28,28)f32
nn.Conv2d                stage3.9.reparam_conv    1 1 60 61 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #60=(1,256,28,28)f32 #61=(1,256,28,28)f32
nn.ReLU                  stage3.9.activation      1 1 61 62 #61=(1,256,28,28)f32 #62=(1,256,28,28)f32
nn.Conv2d                stage3.10.reparam_conv   1 1 62 63 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #62=(1,256,28,28)f32 #63=(1,256,28,28)f32
nn.ReLU                  stage3.10.activation     1 1 63 64 #63=(1,256,28,28)f32 #64=(1,256,28,28)f32
nn.Conv2d                stage3.11.reparam_conv   1 1 64 65 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #64=(1,256,28,28)f32 #65=(1,256,28,28)f32
nn.ReLU                  stage3.11.activation     1 1 65 66 #65=(1,256,28,28)f32 #66=(1,256,28,28)f32
nn.Conv2d                stage3.12.reparam_conv   1 1 66 67 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #66=(1,256,28,28)f32 #67=(1,256,28,28)f32
nn.ReLU                  stage3.12.activation     1 1 67 68 #67=(1,256,28,28)f32 #68=(1,256,28,28)f32
nn.Conv2d                stage3.13.reparam_conv   1 1 68 69 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #68=(1,256,28,28)f32 #69=(1,256,28,28)f32
nn.ReLU                  stage3.13.activation     1 1 69 70 #69=(1,256,28,28)f32 #70=(1,256,28,28)f32
nn.Conv2d                stage3.14.reparam_conv   1 1 70 71 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #70=(1,256,28,28)f32 #71=(1,256,28,28)f32
nn.ReLU                  stage3.14.activation     1 1 71 72 #71=(1,256,28,28)f32 #72=(1,256,28,28)f32
nn.Conv2d                stage3.15.reparam_conv   1 1 72 73 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #72=(1,256,28,28)f32 #73=(1,256,28,28)f32
nn.ReLU                  stage3.15.activation     1 1 73 74 #73=(1,256,28,28)f32 #74=(1,256,28,28)f32
nn.Conv2d                stage3.16.reparam_conv   1 1 74 75 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #74=(1,256,28,28)f32 #75=(1,256,28,28)f32
nn.ReLU                  stage3.16.activation     1 1 75 76 #75=(1,256,28,28)f32 #76=(1,256,28,28)f32
nn.Conv2d                stage3.17.reparam_conv   1 1 76 77 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #76=(1,256,28,28)f32 #77=(1,256,28,28)f32
nn.ReLU                  stage3.17.activation     1 1 77 78 #77=(1,256,28,28)f32 #78=(1,256,28,28)f32
nn.Conv2d                stage3.18.reparam_conv   1 1 78 79 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,1,3,3)f32 #78=(1,256,28,28)f32 #79=(1,256,28,28)f32
nn.ReLU                  stage3.18.activation     1 1 79 80 #79=(1,256,28,28)f32 #80=(1,256,28,28)f32
nn.Conv2d                stage3.19.reparam_conv   1 1 80 81 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,1,1)f32 #80=(1,256,28,28)f32 #81=(1,256,28,28)f32
nn.ReLU                  stage3.19.activation     1 1 81 82 #81=(1,256,28,28)f32 #82=(1,256,28,28)f32
nn.Conv2d                stage4.0.reparam_conv    1 1 82 83 bias=True dilation=(1,1) groups=256 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(256)f32 @weight=(256,1,3,3)f32 #82=(1,256,28,28)f32 #83=(1,256,14,14)f32
nn.ReLU                  stage4.0.activation      1 1 83 84 #83=(1,256,14,14)f32 #84=(1,256,14,14)f32
nn.Conv2d                stage4.1.reparam_conv    1 1 84 85 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=1024 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(1024)f32 @weight=(1024,256,1,1)f32 #84=(1,256,14,14)f32 #85=(1,1024,14,14)f32
nn.ReLU                  stage4.1.activation      1 1 85 86 #85=(1,1024,14,14)f32 #86=(1,1024,14,14)f32
nn.AdaptiveAvgPool2d     gap                      1 1 86 87 output_size=(1,1) #86=(1,1024,14,14)f32 #87=(1,1024,1,1)f32
Tensor.view              Tensor.view_1            1 1 87 88 shape=(1,1024) $input=87 #87=(1,1024,1,1)f32 #88=(1,1024)f32
nn.Linear                fc_yaw                   1 1 88 89 bias=True in_features=1024 out_features=90 @bias=(90)f32 @weight=(90,1024)f32 #88=(1,1024)f32 #89=(1,90)f32
nn.Linear                fc_pitch                 1 1 88 90 bias=True in_features=1024 out_features=90 @bias=(90)f32 @weight=(90,1024)f32 #88=(1,1024)f32 #90=(1,90)f32
prim::TupleConstruct     pnnx_50                  2 1 90 89 91 #90=(1,90)f32 #89=(1,90)f32
pnnx.Output              pnnx_output_0            1 0 91
