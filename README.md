# ğŸ¯ NeuGaze: Facial Expression & Gaze-Based Computer Control

<div align="center">

**[ä¸­æ–‡æ–‡æ¡£](README-CN.md) | English**

[![arXiv](https://img.shields.io/badge/arXiv-2504.15101-b31b1b.svg)](https://arxiv.org/abs/2504.15101)
[![Demo Video](https://img.shields.io/badge/Demo-Bilibili-00A1D6)](https://www.bilibili.com/video/BV1kKdYYVEEM/#reply270100925344)
[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![Platform](https://img.shields.io/badge/Platform-Windows-green.svg)](https://www.microsoft.com/)
[![License](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)

*A non-invasive computer control system that combines facial expression recognition, head movement tracking, and gaze estimation, designed for hands-free human-computer interaction.*

This system enables complex action game control, as demonstrated in the video showing Black Myth: Wukong defeating the Yin Tiger boss. It can also be used to play MOBA games like Honor of Kings, FPS games like CS2, and many other game types.

<div align="center">
    <h3>NeuGaze wukong</h3>
    <video src="https://github.com/user-attachments/assets/2b604e6e-7468-470c-a3df-afc302ffedb0" />
</div>
---

We are hosting a global CS2 Arms Race Challenge: 2000 RMB for the champion. The first participant to complete the setup, achieve kills in Arms Race against bots, and publish a tutorial video will receive an additional 500 RMB bonus.

## ğŸ“‹ Table of Contents

- [ğŸ¯ Overview](#-overview)
- [ğŸ“Š Performance Evaluation](#-performance-evaluation)
- [ğŸš€ Installation](#-installation)
- [ğŸ® Quick Start](#-quick-start)
- [ğŸ˜Š Expression & Control Configuration](#-expression--control-configuration)
- [ğŸ¯ Use Cases](#-use-cases)
- [âš™ï¸ Technical Details](#ï¸-technical-details)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## ğŸ¯ Overview

Traditional assistive technologies face significant limitations: invasive brain-computer interfaces like Neuralink require surgical implantation, commercial eye trackers like Tobii lack precision for complex operations, and traditional assistive devices often involve cumbersome controls. **NeuGaze** addresses these challenges by integrating facial expressions, head movements, and gaze estimation to create an intuitive, hands-free control system.

### ğŸ–¥ï¸ Requirements

| Component | Requirement | Description |
|-----------|-------------|-------------|
| ğŸ“· **Camera** | Standard webcam | No special hardware needed |
| ğŸ’» **Processor** | CPU-only operation | No GPU required |
| ğŸªŸ **OS** | Windows | Other platforms may work but untested |

### â­ Key Features

<div align="center">

| Feature | Description | Icon |
|---------|-------------|------|
| **Gaze-based mouse control** | Real-time calibration, precise tracking | ğŸ¯ |
| **Facial expression mapping** | Keyboard/mouse action mapping | ğŸ˜Š |
| **Three-modal control** | Combining gaze, expressions, and head movements | ğŸ® |
| **Customizable configurations** | Adaptable for different use cases | âš™ï¸ |
| **Real-time performance** | CPU-optimized inference | ğŸš€ |

</div>

---

## ğŸ“Š Performance Evaluation

We conducted comprehensive testing using progressive training on multiple calibration datasets. The system achieves stable gaze tracking performance with the following metrics:

### ğŸ“ˆ Performance Metrics

- **Mean Error**: **48mm** (original)
- **After Kalman Filtering**: **40mm** (optimized)
- **Display Resolution**: 3072Ã—1920
- **Training Data**: Multiple calibration datasets

> ğŸ’¡ **Note**: Current performance is indeed inferior to Tobii's results, but we welcome community collaboration for improvements!

![Progressive Training Analysis](results/progressive_training/comprehensive_comparison.png)

*Progressive training results showing error reduction and performance stability across multiple datasets. The analysis demonstrates consistent improvement in gaze accuracy as more training data is incorporated.*

---

## ğŸš€ Installation

### ğŸ¯ Recommended Installation

#### Windows Users
```cmd
# Double-click to run the installation script
install.bat
```

<details>
<summary>ğŸ“‹ Script Function Details</summary>

The script will automatically:
1. âœ… Check Python version and conda environment
2. âœ… Create a conda environment named `neugaze`
3. âœ… Guide you to activate the environment and install dependencies
4. âœ… Verify successful installation

</details>

### ğŸ”§ Manual Environment Setup

```bash
# Create and activate conda environment
conda create -n neugaze python=3.11.11
conda activate neugaze

# Install all dependencies
pip install -r requirements.txt
```

---

## ğŸ® Quick Start

### ğŸ“¹ Video Tutorial

Watch this video to quickly understand the system usage:

![Quick Start](assets/demo.gif)

### ğŸ“‹ Step-by-Step Guide

#### 1ï¸âƒ£ Launch the GUI

```bash
python config_gui_cpu.py
```

#### 2ï¸âƒ£ Camera Setup

- ğŸ“· Select your camera (default: camera 0)
- ğŸ‘¤ Position your face in the center of the preview window
- âœ… Click "Confirm Selection"

![Camera Selection](assets/camera%20selection.png)

#### 3ï¸âƒ£ Calibration Process

- ğŸ¯ Click "Start Calibration"
- ğŸ‘ï¸ Follow the on-screen dots with your gaze
- ğŸ‘€ Keep your eyes open (photos are only taken when eyes are detected)
- â³ Wait for calibration to complete

#### 4ï¸âƒ£ Start Control

- ğŸ® Click "Start Evaluation"
- ğŸ–±ï¸ Your mouse cursor will now follow your gaze
- ğŸ˜Š Use facial expressions to trigger actions

---

## ğŸ˜Š Expression & Control Configuration

NeuGaze uses a sophisticated expression recognition system defined in `configs/cpu.yaml`. The system supports multiple control modes and customizable mappings.

### ğŸ­ Expression Detection

The system recognizes facial expressions through MediaPipe landmarks and maps them to specific actions:

#### Core Expression Mapping

| Expression | Action Description | Triggered Operation |
|------------|-------------------|---------------------|
| **Open Mouth** (`jawOpen`) | *Drop your jaw naturally* | ğŸ¯ Mode selector - displays available control wheels |
| **Pucker Lips** (`mouthPucker`) | *Make a kissing motion with pursed lips* | ğŸ–±ï¸ Left mouse click |
| **Jaw Left** (`jawLeft`) | *Shift your jaw to the left side* | ğŸ–±ï¸ Right mouse click |
| **Jaw Right** (`jawRight`) | *Shift your jaw to the right side* | ğŸ–±ï¸ Middle mouse click |
| **Smile Left** (`mouthSmileLeft`) | *Smile with only the left side of your mouth* | ğŸ§­ Navigation/selection |
| **Smile Right** (`mouthSmileRight`) | *Smile with only the right side of your mouth* | ğŸ§­ Navigation/selection |
| **Both Sides Smile** | *Full natural smile with both sides* | âš¡ Special commands |
| **Head Movements** | *Tilt, turn, and nod your head* | âŒ¨ï¸ WASD keys and scrolling |

#### Expression Threshold Adjustment

```cmd
python learn\mediapipe_example.py
```

You can use this program to view the scores for different expressions and adjust the thresholds accordingly.

#### Expression Configuration Example

```yaml
left_click:
  conditions:
  - feature: mouthPucker
    operator: '>'
    threshold: 0.97
  - feature: mouthFunnel
    operator: <
    threshold: 0.2
  combine: AND
```

### ğŸ® Control Modes

The system supports multiple operation modes through the wheel interface:

#### ğŸ¯ 1. Game Mode (`game`)

Optimized for gaming with WASD movement and common game keys:

| Wheel Position | Key Mapping | Function Description |
|----------------|-------------|---------------------|
| **num1** | Z/X/C keys | ğŸ® Common game actions |
| **num2** | Shift | ğŸƒ Sprint/crouch |
| **num4** | Number keys 1-4 | âš”ï¸ Weapon selection |
| **num6** | Q/R/F/T keys | ğŸ¯ Interaction keys |
| **num8** | Space | âš¡ Jump |

#### ğŸ¯ 2. CS:GO Mode (`game_cs`)

Specialized for Counter-Strike with tactical bindings:

| Wheel Position | Key Mapping | Function Description |
|----------------|-------------|---------------------|
| **num2** | Space | â¬†ï¸ Jump |
| **num8** | Shift | ğŸš¶ Walk/precision |
| **Mouse lock** | Disabled | ğŸ¯ For precise aiming |

#### ğŸ¯ 3. Honor of Kings Mode (`game_wz`)

Optimized for MOBA gameplay (ç‹è€…è£è€€/Arena of Valor):

| Wheel Position | Key Mapping | Function Description |
|----------------|-------------|---------------------|
| **num1-3** | Skill activation | âš”ï¸ Skills 1-3 |
| **num4** | M key | ğŸ—ºï¸ Map |

#### âŒ¨ï¸ 4. Typing Mode (`type`)

Full keyboard access for text input:

| Wheel Position | Key Mapping | Function Description |
|----------------|-------------|---------------------|
| **num4** | Complete alphabet | ğŸ”¤ Square layout letters |
| **num6** | Numbers and symbols | ğŸ”¢ Square layout numbers and symbols |
| **num2** | Modifier keys | âŒ¨ï¸ Shift, Ctrl, Alt, etc. |
| **num3** | Common shortcuts | ğŸ“‹ Ctrl+C, Ctrl+V, etc. |

### âš™ï¸ Advanced Configuration

#### ğŸ¯ Expression Priorities

The system includes priority rules to prevent conflicting expressions:

```yaml
priority_rules:
- when: num7
  disable: [num2]
  except: []
```

#### ğŸ¨ Wheel Layouts

Different input modes support different wheel layouts:

| Layout Type | Description | Use Case |
|-------------|-------------|----------|
| **Default** | Circular arrangement | ğŸ® Gaming modes |
| **Square** | Grid layout | âŒ¨ï¸ Letter and symbol input |

#### ğŸ¯ Head Movement Integration

Head orientation controls additional functions:

| Head Movement | Key Mapping | Function Description |
|---------------|-------------|---------------------|
| **Pitch (up/down)** | W/S keys | â¬†ï¸â¬‡ï¸ Up/down movement |
| **Yaw (left/right)** | A/D keys | â¬…ï¸â¡ï¸ Left/right movement |
| **Roll (tilt)** | Scroll wheel | ğŸ”„ Scrolling operations |

---

## ğŸ¯ Use Cases

### â™¿ Accessibility

- **ğŸ¦½ Mobility Assistance**: Hands-free computer operation for users with limited mobility
- **ğŸ¥ Rehabilitation**: Motor skill training through controlled head and facial movements

### ğŸ® Gaming & Entertainment

- **ğŸ¯ Immersive Gaming**: Novel input method for enhanced gaming experiences
- **ğŸ’ Action Games**: Complex action game control as demonstrated with Black Myth: Wukong boss battles
- **ğŸ† MOBA Games**: Strategic gameplay in Honor of Kings and similar MOBAs
- **ğŸ¯ FPS Games**: Precision control for Counter-Strike 2 and other competitive shooters
- **ğŸ’ª Muscle Training**: Facial and neck muscle exercise through interactive control

### ğŸ¤– Smart Device Integration

- **ğŸ¥½ AR/VR Interfaces**: Natural control for head-mounted displays
- **ğŸ‘“ Smart Glasses**: Expression-based navigation without hand gestures

---

## âš™ï¸ Technical Details

### ğŸ—ï¸ Architecture

| Component | Function Description | Technical Implementation |
|-----------|---------------------|-------------------------|
| **ğŸ¯ Intent Recognition** | Comprehensive analysis of facial expressions, head movements, and gaze patterns | MediaPipe + custom algorithms |
| **ğŸ”„ Intent Mapping** | Translation of recognized intents into specific keyboard/mouse actions | Configuration-driven mapping system |
| **ğŸ­ Multi-Modal Fusion** | Integration and prioritization of multiple simultaneous intents | Priority rule engine |
| **âš¡ Action Execution** | Coordinated control system enabling complex gaming operations | Real-time control interface |
| **ğŸš€ Optimization** | CPU-optimized inference pipeline for real-time performance | CPU-optimized inference |

### âš ï¸ Limitations

| Limitation | Impact | Solution |
|------------|--------|----------|
| **ğŸŒ Lighting Sensitivity** | Performance degrades in poor or uneven lighting | Adjust environmental lighting |
| **ğŸ¯ Calibration Required** | Individual calibration needed for optimal accuracy | Regular recalibration |
| **ğŸ“š Expression Training** | Learning curve for natural expression control | Practice and adaptation |

---

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit issues, feature requests, or pull requests.

### ğŸ¯ Contribution Methods

- ğŸ› **Report Issues**: Submit bug reports
- ğŸ’¡ **Feature Suggestions**: Propose new feature ideas
- ğŸ”§ **Code Contributions**: Submit pull requests
- ğŸ“š **Documentation Improvements**: Help improve documentation

---

## ğŸ“„ License

[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)

This project is licensed under Creative Commons Attribution-NonCommercial 4.0 International License.

### âœ… You are free to:

- âœ… Share and adapt the code for personal use
- âœ… Use for research and educational purposes
- âœ… Create derivative works for non-commercial purposes (e.g., streaming, content creation)

### âŒ You may NOT:

- âŒ Sell the source code or derivatives
- âŒ Deploy as commercial hardware/software products
- âŒ Package as paid executable applications
- âŒ Use for commercial web services

> ğŸ’¡ **Note**: We provide this software freely to benefit the community while preventing exploitation by commercial entities. License terms may be updated to Apache-2.0 or MIT based on community feedback.

---

## ğŸ“š Citation

```bibtex
@article{yang2024neugaze,
  title={NeuGaze: Facial Expression and Gaze-Based Computer Control},
  author={Yang, Yiqian},
  journal={arXiv preprint arXiv:2504.15101},
  year={2024}
}
```

---

<div align="center">

**âš ï¸ Note**: This system is designed for research and accessibility purposes. While functional, it may require individual tuning for optimal performance. We encourage experimentation and welcome feedback to improve the system's robustness and usability.

---

â­ **If this project helps you, please give us a star!** â­

</div>
